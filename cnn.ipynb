{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/deeplake/util/check_latest_version.py:32: UserWarning: A newer version of deeplake (3.8.4) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import deeplake\n",
    "import numpy as np\n",
    "\n",
    "import tqdm\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening dataset in read-only mode as you don't have write permissions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/activeloop/adience\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hub://activeloop/adience loaded successfully.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening dataset in read-only mode as you don't have write permissions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/activeloop/celeb-a-train\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\\"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hub://activeloop/celeb-a-train loaded successfully.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening dataset in read-only mode as you don't have write permissions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/activeloop/celeb-a-val\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hub://activeloop/celeb-a-val loaded successfully.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening dataset in read-only mode as you don't have write permissions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\\"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/activeloop/celeb-a-test\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\\"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hub://activeloop/celeb-a-test loaded successfully.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " "
     ]
    }
   ],
   "source": [
    "from dataprocessing import DataLoaderWrapper\n",
    "\n",
    "data=DataLoaderWrapper(batch_size=32)\n",
    "celebA_train,celebA_val,celebA_test=data.initialize_celebA_dataloaders()\n",
    "adience=data.initialize_adience_dataloaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Levi_Hassner(nn.Module):\n",
    "    def __init__(self,output=2,deformable=False) -> None:\n",
    "        super().__init__()\n",
    "        self.deformable=deformable\n",
    "\n",
    "        self.layers=nn.Sequential(OrderedDict([\n",
    "            # first convolutional layer\n",
    "            ('conv1',nn.Conv2d(3, 96, 7, padding='valid', stride=4)),  # No padding\n",
    "            ('relu1',nn.ReLU()),\n",
    "            ('maxpool1',nn.MaxPool2d(3, stride=2)),  # Max pooling over a (3, 3) window with 2 pixel stride)\n",
    "            ('lrn1',nn.LocalResponseNorm(size=5, k=2, alpha=10**(-4), beta=0.75)),\n",
    "\n",
    "            # second convolutional layer\n",
    "            ('conv2',nn.Conv2d(96, 256, 5, padding='same')), # Same padding\n",
    "            ('relu2',nn.ReLU()),\n",
    "            ('maxpool2',nn.MaxPool2d(3, stride=2)),  # Max pooling over a (3, 3) window with 2 pixel stride)\n",
    "            ('lrn2',nn.LocalResponseNorm(size=5, k=2, alpha=10**(-4), beta=0.75)),\n",
    "\n",
    "            # third convolutional layer\n",
    "            ('conv3',nn.Conv2d(256, 384, 3, padding='same')),  # Same padding\n",
    "            ('relu3',nn.ReLU()),\n",
    "            ('maxpool3',nn.MaxPool2d(3, stride=2)),  # Max pooling over a (3, 3) window with 2 pixel stride)\n",
    "            ('flatten',nn.Flatten()),\n",
    "\n",
    "            ('fc1',nn.Linear(384*6*6, 512)), # input 384 * 6 * 6 = 13824, output 512\n",
    "            ('relu4',nn.ReLU()),\n",
    "            ('dropout1',nn.Dropout(0.5)),\n",
    "\n",
    "            ('fc2',nn.Linear(512,512)),\n",
    "            ('relu5',nn.ReLU()),\n",
    "            ('dropout2',nn.Dropout(0.5)),\n",
    "            \n",
    "            ('fc3',nn.Linear(512,output)), # output = number of classes \n",
    "        ]))\n",
    "        self.prob=nn.Softmax(dim=1) # new stuff to check if its causes harm\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.layers(x)\n",
    "        prob=self.prob(x)\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Levi_Hassner(\n",
       "  (layers): Sequential(\n",
       "    (conv1): Conv2d(3, 96, kernel_size=(7, 7), stride=(4, 4), padding=valid)\n",
       "    (relu1): ReLU()\n",
       "    (maxpool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (lrn1): LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=2)\n",
       "    (conv2): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=same)\n",
       "    (relu2): ReLU()\n",
       "    (maxpool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (lrn2): LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=2)\n",
       "    (conv3): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "    (relu3): ReLU()\n",
       "    (maxpool3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (fc1): Linear(in_features=13824, out_features=512, bias=True)\n",
       "    (relu4): ReLU()\n",
       "    (dropout1): Dropout(p=0.5, inplace=False)\n",
       "    (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (relu5): ReLU()\n",
       "    (dropout2): Dropout(p=0.5, inplace=False)\n",
       "    (fc3): Linear(in_features=512, out_features=2, bias=True)\n",
       "  )\n",
       "  (prob): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_model = Levi_Hassner()\n",
    "gender_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Levi_Hassner(\n",
       "  (layers): Sequential(\n",
       "    (conv1): Conv2d(3, 96, kernel_size=(7, 7), stride=(4, 4), padding=valid)\n",
       "    (relu1): ReLU()\n",
       "    (maxpool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (lrn1): LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=2)\n",
       "    (conv2): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=same)\n",
       "    (relu2): ReLU()\n",
       "    (maxpool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (lrn2): LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=2)\n",
       "    (conv3): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "    (relu3): ReLU()\n",
       "    (maxpool3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (fc1): Linear(in_features=13824, out_features=512, bias=True)\n",
       "    (relu4): ReLU()\n",
       "    (dropout1): Dropout(p=0.5, inplace=False)\n",
       "    (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (relu5): ReLU()\n",
       "    (dropout2): Dropout(p=0.5, inplace=False)\n",
       "    (fc3): Linear(in_features=512, out_features=8, bias=True)\n",
       "  )\n",
       "  (prob): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "age_model = Levi_Hassner(output=8)\n",
    "age_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/deeplake/integrations/pytorch/common.py:126: UserWarning: Decode method for tensors ['images'] is defaulting to numpy. Please consider specifying a decode_method in .pytorch() that maximizes the data preprocessing speed based on your transformation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 227, 227])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 3, 227, 227])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 3, 227, 227])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 3, 227, 227])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 3, 227, 227])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 3, 227, 227])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 3, 227, 227])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 3, 227, 227])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 3, 227, 227])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 3, 227, 227])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 3, 227, 227])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 3, 227, 227])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 3, 227, 227])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 3, 227, 227])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 3, 227, 227])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 3, 227, 227])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 3, 227, 227])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 3, 227, 227])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 3, 227, 227])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 3, 227, 227])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 3, 227, 227])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 3, 227, 227])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 3, 227, 227])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 3, 227, 227])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 3, 227, 227])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 3, 227, 227])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 3, 227, 227])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/remeliashirlley/Documents/GitHub/CZ4042-gender-classification/cnn.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/remeliashirlley/Documents/GitHub/CZ4042-gender-classification/cnn.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m i,data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(celebA_train):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/remeliashirlley/Documents/GitHub/CZ4042-gender-classification/cnn.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mprint\u001b[39m(data[\u001b[39m'\u001b[39m\u001b[39mimages\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mshape)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/remeliashirlley/Documents/GitHub/CZ4042-gender-classification/cnn.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39m(data[\u001b[39m'\u001b[39m\u001b[39mmale\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:32\u001b[0m, in \u001b[0;36m_IterableDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m possibly_batched_index:\n\u001b[1;32m     31\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 32\u001b[0m         data\u001b[39m.\u001b[39mappend(\u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset_iter))\n\u001b[1;32m     33\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mended \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/deeplake/integrations/pytorch/dataset.py:167\u001b[0m, in \u001b[0;36mTorchDataset.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    163\u001b[0m     schedule\u001b[39m.\u001b[39mshuffle()\n\u001b[1;32m    165\u001b[0m stream \u001b[39m=\u001b[39m streaming\u001b[39m.\u001b[39mread(schedule)\n\u001b[0;32m--> 167\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m stream:\n\u001b[1;32m    168\u001b[0m     \u001b[39myield\u001b[39;00m _process(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_index)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/deeplake/core/io.py:351\u001b[0m, in \u001b[0;36mSampleStreaming.read\u001b[0;34m(self, schedule)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread\u001b[39m(\u001b[39mself\u001b[39m, schedule: Schedule) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator:\n\u001b[1;32m    350\u001b[0m     \u001b[39mfor\u001b[39;00m block \u001b[39min\u001b[39;00m schedule\u001b[39m.\u001b[39m_blocks:\n\u001b[0;32m--> 351\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream(block)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/deeplake/core/io.py:396\u001b[0m, in \u001b[0;36mSampleStreaming.stream\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m    394\u001b[0m     chunks\u001b[39m.\u001b[39mappend(chunk)\n\u001b[1;32m    395\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(chunks) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 396\u001b[0m     data \u001b[39m=\u001b[39m engine\u001b[39m.\u001b[39;49mread_sample_from_chunk(\n\u001b[1;32m    397\u001b[0m         idx, chunk, decompress\u001b[39m=\u001b[39;49mdecompress, to_pil\u001b[39m=\u001b[39;49mto_pil\n\u001b[1;32m    398\u001b[0m     )\n\u001b[1;32m    399\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    400\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m decompress:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/deeplake/core/chunk_engine.py:1843\u001b[0m, in \u001b[0;36mChunkEngine.read_sample_from_chunk\u001b[0;34m(self, global_sample_index, chunk, cast, copy, decompress, to_pil)\u001b[0m\n\u001b[1;32m   1834\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(chunk, SampleCompressedChunk)\n\u001b[1;32m   1835\u001b[0m     \u001b[39mreturn\u001b[39;00m chunk\u001b[39m.\u001b[39mread_sample(\n\u001b[1;32m   1836\u001b[0m         local_sample_index,\n\u001b[1;32m   1837\u001b[0m         cast\u001b[39m=\u001b[39mcast,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1840\u001b[0m         to_pil\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   1841\u001b[0m     )\n\u001b[0;32m-> 1843\u001b[0m \u001b[39mreturn\u001b[39;00m chunk\u001b[39m.\u001b[39;49mread_sample(\n\u001b[1;32m   1844\u001b[0m     local_sample_index, cast\u001b[39m=\u001b[39;49mcast, copy\u001b[39m=\u001b[39;49mcopy, decompress\u001b[39m=\u001b[39;49mdecompress\n\u001b[1;32m   1845\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/deeplake/core/chunk/base_chunk.py:632\u001b[0m, in \u001b[0;36mcatch_chunk_read_error.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[39m@wraps\u001b[39m(fn)\n\u001b[1;32m    630\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    631\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 632\u001b[0m         \u001b[39mreturn\u001b[39;00m fn(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    633\u001b[0m     \u001b[39mexcept\u001b[39;00m EmptyTensorError:\n\u001b[1;32m    634\u001b[0m         \u001b[39mraise\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/deeplake/core/chunk/sample_compressed_chunk.py:138\u001b[0m, in \u001b[0;36mSampleCompressedChunk.read_sample\u001b[0;34m(self, local_index, cast, copy, sub_index, stream, decompress, is_tile, to_pil)\u001b[0m\n\u001b[1;32m    131\u001b[0m     buffer \u001b[39m=\u001b[39m decompress_bytes(buffer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompression)\n\u001b[1;32m    132\u001b[0m     \u001b[39mreturn\u001b[39;00m Polygons\u001b[39m.\u001b[39mfrombuffer(\n\u001b[1;32m    133\u001b[0m         \u001b[39mbytes\u001b[39m(buffer),\n\u001b[1;32m    134\u001b[0m         dtype\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtensor_meta\u001b[39m.\u001b[39mdtype,\n\u001b[1;32m    135\u001b[0m         ndim\u001b[39m=\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m],\n\u001b[1;32m    136\u001b[0m     )\n\u001b[0;32m--> 138\u001b[0m sample \u001b[39m=\u001b[39m decompress_array(\n\u001b[1;32m    139\u001b[0m     buffer,\n\u001b[1;32m    140\u001b[0m     shape,\n\u001b[1;32m    141\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdtype,\n\u001b[1;32m    142\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompression,\n\u001b[1;32m    143\u001b[0m     start_idx\u001b[39m=\u001b[39;49mstart,\n\u001b[1;32m    144\u001b[0m     end_idx\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    145\u001b[0m     step\u001b[39m=\u001b[39;49mstep,\n\u001b[1;32m    146\u001b[0m     reverse\u001b[39m=\u001b[39;49mreverse,\n\u001b[1;32m    147\u001b[0m     to_pil\u001b[39m=\u001b[39;49mto_pil,\n\u001b[1;32m    148\u001b[0m )\n\u001b[1;32m    149\u001b[0m \u001b[39mif\u001b[39;00m to_pil:\n\u001b[1;32m    150\u001b[0m     \u001b[39mreturn\u001b[39;00m sample\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/deeplake/core/compression.py:336\u001b[0m, in \u001b[0;36mdecompress_array\u001b[0;34m(buffer, shape, dtype, compression, start_idx, end_idx, step, reverse, to_pil, path)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[39mif\u001b[39;00m to_pil:\n\u001b[1;32m    335\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n\u001b[0;32m--> 336\u001b[0m arr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(img)\n\u001b[1;32m    337\u001b[0m \u001b[39mif\u001b[39;00m shape \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     arr \u001b[39m=\u001b[39m arr\u001b[39m.\u001b[39mreshape(shape)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/PIL/Image.py:673\u001b[0m, in \u001b[0;36mImage.__array_interface__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m         new[\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtobytes(\u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mL\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    672\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 673\u001b[0m         new[\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtobytes()\n\u001b[1;32m    674\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(e, (\u001b[39mMemoryError\u001b[39;00m, \u001b[39mRecursionError\u001b[39;00m)):\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/PIL/Image.py:732\u001b[0m, in \u001b[0;36mImage.tobytes\u001b[0;34m(self, encoder_name, *args)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[39mif\u001b[39;00m encoder_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m args \u001b[39m==\u001b[39m ():\n\u001b[1;32m    730\u001b[0m     args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode\n\u001b[0;32m--> 732\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m    734\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwidth \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheight \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    735\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/PIL/ImageFile.py:269\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(msg)\n\u001b[1;32m    268\u001b[0m b \u001b[39m=\u001b[39m b \u001b[39m+\u001b[39m s\n\u001b[0;32m--> 269\u001b[0m n, err_code \u001b[39m=\u001b[39m decoder\u001b[39m.\u001b[39;49mdecode(b)\n\u001b[1;32m    270\u001b[0m \u001b[39mif\u001b[39;00m n \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    271\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i,data in enumerate(celebA_train):\n",
    "    print(data['images'].shape)\n",
    "    print(data['male'].shape)\n",
    "    print(data['young'].shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 53 * 53, 120)  # Adjust the input dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)  # Flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "net=Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Levi_Hassner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/deeplake/integrations/pytorch/common.py:126: UserWarning: Decode method for tensors ['images'] is defaulting to numpy. Please consider specifying a decode_method in .pytorch() that maximizes the data preprocessing speed based on your transformation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6783455014228821\n",
      "1.3590947985649109\n",
      "2.036963641643524\n",
      "0.681087851524353\n",
      "1.3592827320098877\n",
      "2.036513566970825\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(gender_model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(2):\n",
    "    running_loss=0.0\n",
    "    for i,data in enumerate(celebA_train):\n",
    "        inputs, labels = data['images'],data['male']\n",
    "        labels=torch.argmax(labels,dim=1)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        print(running_loss)\n",
    "        if i==2:break\n",
    "        # if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "        #     print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "        #     running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/deeplake/integrations/pytorch/common.py:126: UserWarning: Decode method for tensors ['images'] is defaulting to numpy. Please consider specifying a decode_method in .pytorch() that maximizes the data preprocessing speed based on your transformation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/remeliashirlley/Documents/GitHub/CZ4042-gender-classification/cnn.ipynb Cell 8\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/remeliashirlley/Documents/GitHub/CZ4042-gender-classification/cnn.ipynb#X10sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m train_steps \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/remeliashirlley/Documents/GitHub/CZ4042-gender-classification/cnn.ipynb#X10sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m test_steps \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/remeliashirlley/Documents/GitHub/CZ4042-gender-classification/cnn.ipynb#X10sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mfor\u001b[39;00m images, age_labels, gender_labels \u001b[39min\u001b[39;00m celebA_train:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/remeliashirlley/Documents/GitHub/CZ4042-gender-classification/cnn.ipynb#X10sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     l, a \u001b[39m=\u001b[39m train_step(images, gender_labels, optimizer, gender_model)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/remeliashirlley/Documents/GitHub/CZ4042-gender-classification/cnn.ipynb#X10sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m l\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:32\u001b[0m, in \u001b[0;36m_IterableDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m possibly_batched_index:\n\u001b[1;32m     31\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 32\u001b[0m         data\u001b[39m.\u001b[39mappend(\u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset_iter))\n\u001b[1;32m     33\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mended \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/deeplake/integrations/pytorch/dataset.py:167\u001b[0m, in \u001b[0;36mTorchDataset.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    163\u001b[0m     schedule\u001b[39m.\u001b[39mshuffle()\n\u001b[1;32m    165\u001b[0m stream \u001b[39m=\u001b[39m streaming\u001b[39m.\u001b[39mread(schedule)\n\u001b[0;32m--> 167\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m stream:\n\u001b[1;32m    168\u001b[0m     \u001b[39myield\u001b[39;00m _process(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_index)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/deeplake/core/io.py:351\u001b[0m, in \u001b[0;36mSampleStreaming.read\u001b[0;34m(self, schedule)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread\u001b[39m(\u001b[39mself\u001b[39m, schedule: Schedule) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator:\n\u001b[1;32m    350\u001b[0m     \u001b[39mfor\u001b[39;00m block \u001b[39min\u001b[39;00m schedule\u001b[39m.\u001b[39m_blocks:\n\u001b[0;32m--> 351\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream(block)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/deeplake/core/io.py:393\u001b[0m, in \u001b[0;36mSampleStreaming.stream\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m    391\u001b[0m             local_cache\u001b[39m.\u001b[39m_forward(c_key)\n\u001b[1;32m    392\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 393\u001b[0m         chunk \u001b[39m=\u001b[39m engine\u001b[39m.\u001b[39;49mget_chunk(c_key)\n\u001b[1;32m    394\u001b[0m     chunks\u001b[39m.\u001b[39mappend(chunk)\n\u001b[1;32m    395\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(chunks) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/deeplake/core/chunk_engine.py:572\u001b[0m, in \u001b[0;36mChunkEngine.get_chunk\u001b[0;34m(self, chunk_key, partial_chunk_bytes)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_chunk\u001b[39m(\u001b[39mself\u001b[39m, chunk_key: \u001b[39mstr\u001b[39m, partial_chunk_bytes\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BaseChunk:\n\u001b[0;32m--> 572\u001b[0m     chunk \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcache\u001b[39m.\u001b[39;49mget_deeplake_object(\n\u001b[1;32m    573\u001b[0m         chunk_key,\n\u001b[1;32m    574\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchunk_class,\n\u001b[1;32m    575\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchunk_args,\n\u001b[1;32m    576\u001b[0m         partial_bytes\u001b[39m=\u001b[39;49mpartial_chunk_bytes,\n\u001b[1;32m    577\u001b[0m     )\n\u001b[1;32m    578\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m partial_chunk_bytes \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(chunk\u001b[39m.\u001b[39mdata_bytes, PartialReader):\n\u001b[1;32m    579\u001b[0m         chunk\u001b[39m.\u001b[39m_make_data_bytearray()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/deeplake/core/storage/lru_cache.py:165\u001b[0m, in \u001b[0;36mLRUCache.get_deeplake_object\u001b[0;34m(self, path, expected_class, meta, url, partial_bytes)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    162\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mExpected class should be subclass of BaseChunk when url is True.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    163\u001b[0m         )\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     item \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m[path]\n\u001b[1;32m    167\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(item, DeepLakeMemoryObject):\n\u001b[1;32m    168\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(item) \u001b[39m!=\u001b[39m expected_class:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/deeplake/core/storage/lru_cache.py:211\u001b[0m, in \u001b[0;36mLRUCache.__getitem__\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnext_storage \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         \u001b[39m# fetch from storage, may throw KeyError\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext_storage[path]\n\u001b[1;32m    213\u001b[0m         \u001b[39mif\u001b[39;00m _get_nbytes(result) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache_size:  \u001b[39m# insert in cache if it fits\u001b[39;00m\n\u001b[1;32m    214\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_insert_in_cache(path, result)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/deeplake/core/storage/s3.py:218\u001b[0m, in \u001b[0;36mS3Provider.__getitem__\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, path):\n\u001b[1;32m    206\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Gets the object present at the path.\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \n\u001b[1;32m    208\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[39m        S3GetError: Any other error other than KeyError while retrieving the object.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_bytes(path)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/deeplake/core/storage/s3.py:261\u001b[0m, in \u001b[0;36mS3Provider.get_bytes\u001b[0;34m(self, path, start_byte, end_byte)\u001b[0m\n\u001b[1;32m    259\u001b[0m path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpath, path))\n\u001b[1;32m    260\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 261\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_bytes(path, start_byte, end_byte)\n\u001b[1;32m    262\u001b[0m \u001b[39mexcept\u001b[39;00m botocore\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mClientError \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    263\u001b[0m     \u001b[39mif\u001b[39;00m err\u001b[39m.\u001b[39mresponse[\u001b[39m\"\u001b[39m\u001b[39mError\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mCode\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mNoSuchKey\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/deeplake/core/storage/s3.py:234\u001b[0m, in \u001b[0;36mS3Provider._get_bytes\u001b[0;34m(self, path, start_byte, end_byte)\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[39mrange\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    233\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mget_object(Bucket\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbucket, Key\u001b[39m=\u001b[39mpath, Range\u001b[39m=\u001b[39m\u001b[39mrange\u001b[39m)\n\u001b[0;32m--> 234\u001b[0m \u001b[39mreturn\u001b[39;00m resp[\u001b[39m\"\u001b[39;49m\u001b[39mBody\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mread()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/botocore/response.py:99\u001b[0m, in \u001b[0;36mStreamingBody.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Read at most amt bytes from the stream.\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \n\u001b[1;32m     96\u001b[0m \u001b[39mIf the amt argument is omitted, read all data.\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 99\u001b[0m     chunk \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raw_stream\u001b[39m.\u001b[39;49mread(amt)\n\u001b[1;32m    100\u001b[0m \u001b[39mexcept\u001b[39;00m URLLib3ReadTimeoutError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    101\u001b[0m     \u001b[39m# TODO: the url will be None as urllib3 isn't setting it yet\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     \u001b[39mraise\u001b[39;00m ReadTimeoutError(endpoint_url\u001b[39m=\u001b[39me\u001b[39m.\u001b[39murl, error\u001b[39m=\u001b[39me)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/urllib3/response.py:879\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    876\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m amt:\n\u001b[1;32m    877\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer\u001b[39m.\u001b[39mget(amt)\n\u001b[0;32m--> 879\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raw_read(amt)\n\u001b[1;32m    881\u001b[0m flush_decoder \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    882\u001b[0m \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/urllib3/response.py:814\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    811\u001b[0m fp_closed \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp, \u001b[39m\"\u001b[39m\u001b[39mclosed\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    813\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 814\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp_read(amt) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fp_closed \u001b[39melse\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    815\u001b[0m     \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m data:\n\u001b[1;32m    816\u001b[0m         \u001b[39m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    817\u001b[0m         \u001b[39m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    822\u001b[0m         \u001b[39m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m         \u001b[39m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    824\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/urllib3/response.py:799\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    796\u001b[0m     \u001b[39mreturn\u001b[39;00m buffer\u001b[39m.\u001b[39mgetvalue()\n\u001b[1;32m    797\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    798\u001b[0m     \u001b[39m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 799\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mread(amt) \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp\u001b[39m.\u001b[39;49mread()\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:482\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    481\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 482\u001b[0m         s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_safe_read(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlength)\n\u001b[1;32m    483\u001b[0m     \u001b[39mexcept\u001b[39;00m IncompleteRead:\n\u001b[1;32m    484\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:631\u001b[0m, in \u001b[0;36mHTTPResponse._safe_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_safe_read\u001b[39m(\u001b[39mself\u001b[39m, amt):\n\u001b[1;32m    625\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Read the number of bytes requested.\u001b[39;00m\n\u001b[1;32m    626\u001b[0m \n\u001b[1;32m    627\u001b[0m \u001b[39m    This function should be used when <amt> bytes \"should\" be present for\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[39m    reading. If the bytes are truly not available (due to EOF), then the\u001b[39;00m\n\u001b[1;32m    629\u001b[0m \u001b[39m    IncompleteRead exception can be used to detect the problem.\u001b[39;00m\n\u001b[1;32m    630\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp\u001b[39m.\u001b[39mread(amt)\n\u001b[1;32m    632\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(data) \u001b[39m<\u001b[39m amt:\n\u001b[1;32m    633\u001b[0m         \u001b[39mraise\u001b[39;00m IncompleteRead(data, amt\u001b[39m-\u001b[39m\u001b[39mlen\u001b[39m(data))\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    707\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:1311\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1307\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1308\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1309\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1310\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1311\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1312\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1313\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:1167\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1165\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1166\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1167\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1168\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1169\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Choose a loss function for training\n",
    "# gender_model = Levi_Hassner(output=1)\n",
    "loss_object = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(gender_model.parameters(), lr=1e-3)\n",
    "EPOCHS = 30\n",
    "\n",
    "# Train step\n",
    "def train_step(images, labels, optimizer, model):\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(images)\n",
    "    predictions = predictions[:, 0].unsqueeze(1)\n",
    "    \n",
    "    loss = loss_object(predictions.float(), labels.float())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    predictions = (predictions >= 0.5).float()\n",
    "    return loss.item(), (predictions == labels).type(torch.float).mean().item()\n",
    "\n",
    "# def test_step(images, labels, model):\n",
    "#     predictions = model(images)\n",
    "#     predictions = predictions[:, 0].unsqueeze(1)\n",
    "#     t_loss = loss_object(predictions.float(), labels.float())\n",
    "#     predictions = (predictions >= 0.5).float()\n",
    "#     return t_loss.item(), (predictions == labels).type(torch.float).mean().item()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "        train_loss = 0.0\n",
    "        train_accuracy = 0.0\n",
    "        test_loss = 0.0\n",
    "        test_accuracy = 0.0\n",
    "        train_steps = 0\n",
    "        test_steps = 0\n",
    "\n",
    "        for images, age_labels, gender_labels in celebA_train:\n",
    "            l, a = train_step(images, gender_labels, optimizer, gender_model)\n",
    "            train_loss += l\n",
    "            train_accuracy += a\n",
    "            train_steps += 1\n",
    "\n",
    "        # with torch.no_grad():\n",
    "        #     for test_images, age_labels, gender_labels in celebA_val:\n",
    "        #         t_l, t_a = test_step(test_images, gender_labels, gender_model)\n",
    "        #         test_loss += t_l\n",
    "        #         test_accuracy += t_a\n",
    "        #         test_steps += 1\n",
    "        #         test_acc.append(test_accuracy / test_steps)\n",
    "        # template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
    "        # print(template.format(epoch + 1,\n",
    "        #                       train_loss / train_steps,\n",
    "        #                       train_accuracy / train_steps,\n",
    "        #                       test_loss / test_steps,\n",
    "        #                       test_accuracy / test_steps))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in gender_model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in gender_model.fc3.parameters(): # unfreeze weights of last layer\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(gender_model.fc3.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = resnet18(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch {epoch+1}/5, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
